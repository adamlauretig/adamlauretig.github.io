<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adam Lauretig on Adam Lauretig</title>
    <link>adamlauretig.github.io/</link>
    <description>Recent content in Adam Lauretig on Adam Lauretig</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="adamlauretig.github.io/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Identification and Inference in Bayesian Word Embeddings</title>
      <link>adamlauretig.github.io/publication/bwe/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 -0400</pubDate>
      
      <guid>adamlauretig.github.io/publication/bwe/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tokenizing on Stopwords</title>
      <link>adamlauretig.github.io/post/tokenizing-on-stopwords/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>adamlauretig.github.io/post/tokenizing-on-stopwords/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Recently, I came across &lt;a href=&#34;https://github.com/dperezrada/evidence-tools/tree/master/nlp/keywords2vec&#34;&gt;the idea&lt;/a&gt; that you can get relevant keywords for word2vec by tokenizing a corpus on stopwords, in addition to standard punctuation (found &lt;a href=&#34;https://twitter.com/jeremyphoward/status/1094025901371621376&#34;&gt;via&lt;/a&gt;). This seemed like a really cool unsupervised way of capturing (hopefully!) relevant phrases. I was intrigued.&lt;/p&gt;
&lt;p&gt;A brief note: “tokenizing” refers to splitting a document into words or phrases based on a pre-defined set of rules. The most common way to do this is by splitting on spaces and “end-of-sentence” punctuation (ex: “!, ?, .”). This would then return a list of words, or “unigrams.” These can then be re-combined into &lt;em&gt;n-grams&lt;/em&gt;, or multiword phrases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-r-implementation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An R Implementation&lt;/h1&gt;
&lt;p&gt;The code in the linked repo was all in Python, but, like many political scientists, I’m more comfortable in R. (This is not a statement on the relative merits of each language, simply my own comfort level. Please, no language wars here!). I was curious whether it would be possible to implement this approach in R.&lt;/p&gt;
&lt;p&gt;My first thought was to use the amazing &lt;em&gt;quanteda&lt;/em&gt; package, and I &lt;a href=&#34;https://github.com/quanteda/quanteda/issues/1600&#34;&gt;asked&lt;/a&gt; the maintainers about this approach. &lt;a href=&#34;https://github.com/kbenoit&#34;&gt;Ken Benoit&lt;/a&gt; had some helpful suggestions, as well as a nice discussion about the &lt;em&gt;quanteda&lt;/em&gt; design philosophy. This approach first tokenizes the corpus, removes stopwords, creates phrases from the remaining words, and finally, recombines them with the original &lt;em&gt;tokens&lt;/em&gt; object. This object can then be used in word2vec or other approaches.&lt;/p&gt;
&lt;p&gt;Here, I test the approach on the US presidential inauagural addresses corpus in &lt;em&gt;quanteda&lt;/em&gt;. I like this corpus because it’s small enough that computation is quick, and there is enough thematic structure that human inspection can tell if the results “make sense.”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(stringsAsFactors = FALSE)
library(quanteda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Package version: 1.5.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parallel computing: 2 of 8 threads used.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See https://quanteda.io for tutorials and examples.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;quanteda&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:utils&amp;#39;:
## 
##     View&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(stringr)
# creating text
d &amp;lt;- quanteda::data_corpus_inaugural
tmp_data &amp;lt;- data.table::data.table(txts = d$documents$texts, yr = d$documents$Year, 
  president = paste(d$documents$FirstName, d$documents$President))
tmp_data &amp;lt;- as.data.table(tmp_data)

# tokenize the text
inaug_tokens &amp;lt;- tokens(x = char_tolower(tmp_data$txts), &amp;quot;word&amp;quot;, 
  remove_punct = TRUE, remove_separators = TRUE)

# remove stopwords, but preserve the initial structure
inaug_tokens2 &amp;lt;- tokens_remove(inaug_tokens, stopwords(&amp;quot;english&amp;quot;), padding = TRUE)

# collocations which are either two or three words long, and occur at least twice
cols &amp;lt;-  textstat_collocations(inaug_tokens2, size = 2, min_count = 2)
# filter collocation
cols &amp;lt;- cols[cols$z &amp;gt;= 2.58 ]
# recombine results
combined &amp;lt;- tokens_compound(inaug_tokens, cols)

# first twenty tokens of Obama&amp;#39;s first inaugural: 
combined[[56]][1:20]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;my&amp;quot;              &amp;quot;fellow_citizens&amp;quot; &amp;quot;i&amp;quot;              
##  [4] &amp;quot;stand&amp;quot;           &amp;quot;here&amp;quot;            &amp;quot;today&amp;quot;          
##  [7] &amp;quot;humbled&amp;quot;         &amp;quot;by&amp;quot;              &amp;quot;the&amp;quot;            
## [10] &amp;quot;task&amp;quot;            &amp;quot;before&amp;quot;          &amp;quot;us&amp;quot;             
## [13] &amp;quot;grateful&amp;quot;        &amp;quot;for&amp;quot;             &amp;quot;the&amp;quot;            
## [16] &amp;quot;trust&amp;quot;           &amp;quot;you&amp;quot;             &amp;quot;have&amp;quot;           
## [19] &amp;quot;bestowed&amp;quot;        &amp;quot;mindful&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;word2vec&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Word2vec&lt;/h1&gt;
&lt;p&gt;Now that we have these recombined tokens, we can use them in our statistical model of choice. Here, I used word2vec, in line with the idea that inspired me. I use the &lt;em&gt;reticulate&lt;/em&gt; package to call the gensim implementation of word2vec, which I initially discussed &lt;a href=&#34;https://github.com/adamlauretig/gensim_in_R/blob/master/gensim_in_r.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(216)
library(reticulate)
gensim &amp;lt;- import(&amp;quot;gensim&amp;quot;) # import the gensim library
Word2Vec &amp;lt;- gensim$models$Word2Vec # Extract the Word2Vec model
multiprocessing &amp;lt;- import(&amp;quot;multiprocessing&amp;quot;) # For parallel processing

# create the word2vec object
basemodel = Word2Vec(
    workers = 1, # using 1 core
    window = 5L, # co-occurence window of size 5
    iter = 10L, # iter = sweeps of SGD through the data; more is better
    sg = 1L,
    hs = 0L, negative = 1L, 
    size = 25L # only 25, since it&amp;#39;s a small corpus
)

# remove quanteda &amp;quot;textnn&amp;quot; names
combined_list &amp;lt;- unname(as.list(combined))

basemodel$build_vocab(sentences = combined_list)
basemodel$train(
  sentences = combined_list,
  epochs = basemodel$iter, 
  total_examples = basemodel$corpus_count)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 725200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, we can examine the model output. We’ll need to bring the embedding matrix into R (again, see my gensim tutorial for details), and use cosine similarity to see how well this captures meaningful relationships:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Matrix)
embeds &amp;lt;- basemodel$wv$syn0
rownames(embeds) &amp;lt;- basemodel$wv$index2word

# function for cosine distance
closest_vector &amp;lt;- function(vec1, mat1){
  vec1 &amp;lt;- Matrix(vec1, nrow = 1, ncol = length(vec1))
  mat1 &amp;lt;- Matrix(mat1)
  mat_magnitudes &amp;lt;- rowSums(mat1^2)
  vec_magnitudes &amp;lt;- rowSums(vec1^2)
  sim &amp;lt;- (t(tcrossprod(vec1, mat1)/
      (sqrt(tcrossprod(vec_magnitudes, mat_magnitudes)))))
  sim2 &amp;lt;- matrix(sim, dimnames = list(rownames(sim)))
  
  w &amp;lt;- sim2[order(-sim2),,drop = FALSE]
  w[1:10,]
}

closest_vector(embeds[&amp;quot;united_states&amp;quot;, ], embeds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      united_states       constitution            article 
##          1.0000000          0.8954939          0.8729917 
##     accountability general_government        legislative 
##          0.8634663          0.8520080          0.8464613 
##            several              under            defects 
##          0.8458755          0.8290494          0.8141552 
##             policy 
##          0.8125987&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;closest_vector(embeds[&amp;quot;americans&amp;quot;, ], embeds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  americans      quiet    resolve       live       face     hearts 
##  1.0000000  0.9407097  0.9357236  0.9310781  0.9199575  0.9130146 
## determined      renew   together     voices 
##  0.9118831  0.9114557  0.9087178  0.9079879&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not bad for an initial analysis!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Monthly Militarized Interstate Dispute (MID) Panel </title>
      <link>adamlauretig.github.io/post/monthly-militarized-interstate-dispute-mid-panel/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>adamlauretig.github.io/post/monthly-militarized-interstate-dispute-mid-panel/</guid>
      <description>&lt;p&gt;Just a brief note. For my dissertation, I needed to create a monthly panel of militarized interstate dispute (MID) data, which proved to be more difficult than initially anticipated. To “be kind to my future self,” I wrote up what I did on a &lt;a href=&#34;https://gist.github.com/adamlauretig/ce536aefe21523e2757ee4f261242b0c&#34;&gt;gist&lt;/a&gt;, and I’ll also copy the code below. If you find yourself in a similar situation, I hope this helps!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Code to create a monthly, directed dyadic panel of 
# Militarized Interstate Dispute data, with directional initiation
# uses cshapes to create monthly list of all dyads, then uses data.table
# to transform MIDS to a monthly panel and then merge

# Code by Adam Lauretig, 2019
# website: adamlauretig.github.io

rm(list = ls())
options(stringsAsFactors = FALSE)
seed_to_use &amp;lt;- 216
set.seed(seed_to_use)
library(data.table)
library(parallel)
library(cshapes)
load(&amp;quot;master_dyads_1946_1980.rdata&amp;quot;) 
# for code to create this, see appendix at bottom
setDT(master_dyads)
# revised MID data from http://svmiller.com/gml-mid-data/
mids &amp;lt;- fread(&amp;quot;gml-ddy-2.1.csv&amp;quot;)

# gets dispute start days ----
mida &amp;lt;- fread(&amp;quot;gml-mida-2.1.csv&amp;quot;)
mida &amp;lt;- mida[,.(
  dispnum3, stday, stmon, styear, endday, endmon, endyear, fatality, hostlev)]
mida &amp;lt;- mida[ styear &amp;gt; 1945 &amp;amp; styear &amp;lt; 1981]

# use &amp;quot;sprintf&amp;quot; to make sure all days/months preserve the leading zero 
mida[, stmon := sprintf(&amp;quot;%02s&amp;quot;, stmon)]
mida[, stday := sprintf(&amp;quot;%02s&amp;quot;, stday)]
mida[, endmon := sprintf(&amp;quot;%02s&amp;quot;, endmon)]
mida[, endday := sprintf(&amp;quot;%02s&amp;quot;, endday)]
mida[, `:=`(endday = ifelse(endday == &amp;quot;-9&amp;quot;, &amp;quot;01&amp;quot;, endday), 
  stday = ifelse(stday == &amp;quot;-9&amp;quot;, &amp;quot;01&amp;quot;, stday))]

# working w/mids ----
# year range of interest
mids &amp;lt;- mids[ year &amp;gt; 1945 &amp;amp; year &amp;lt; 1981]
mids[, fatal_mid := as.numeric(!(is.na(fatality)) &amp;amp; fatality &amp;gt; 0) ]
mids_sub &amp;lt;- mids[,.(year, ccode1, ccode2, hostlev, sidea1, sidea2, dispnum)]

# merge dispute data with dyad data ----
mid_data &amp;lt;- merge(
  mids_sub, mida, by.x = c(&amp;quot;dispnum&amp;quot;), by.y = c(&amp;quot;dispnum3&amp;quot;), all.x = TRUE)
# since we&amp;#39;re working w/months, set all start dates to &amp;quot;YYYY-MM-01&amp;quot;
mid_data[, start_date := paste0(styear, &amp;quot;-&amp;quot;, stmon, &amp;quot;-&amp;quot;, &amp;quot;01&amp;quot;)]
mid_data[, end_date := paste0(endyear, &amp;quot;-&amp;quot;, endmon, &amp;quot;-&amp;quot;, &amp;quot;01&amp;quot;)]
# clear out some unneeded columns
mid_data[, `:=`(stday = NULL, stmon = NULL, styear = NULL, endday = NULL, 
  endmon = NULL, endyear = NULL, hostlev.y = NULL)]
# here, we only want to look at valid disputes, we&amp;#39;re removing any odd ducks
disputes &amp;lt;- mid_data[ !(is.na(dispnum))]
disputes &amp;lt;- disputes[ !(is.na(fatality)) ]
dispute_nums &amp;lt;- unique(disputes$dispnum)


# take the &amp;quot;from&amp;quot; and &amp;quot;to&amp;quot; columns, and create a monthly panel, and 
# code onsets and initiations. returns a data.table
make_monthly_mid &amp;lt;- function(i, dispute_dt = disputes){
  disp &amp;lt;- dispute_nums[i]
  tmp &amp;lt;- dispute_dt[ dispnum == disp ]
  tmp2 &amp;lt;- tmp[ , list(month = seq(as.Date(start_date[1]), 
    as.Date(end_date[1]), by = &amp;quot;month&amp;quot;)), 
    by = .(dispnum, ccode1, ccode2, hostlev.x, sidea1, sidea2, fatality)]
  tmp2[, onset := ifelse(month == min(month), 1, 0)]
  tmp2[, onset_na := ifelse(month == min(month), 1, NA)]
  tmp2[, init := ifelse(onset == 1 &amp;amp; sidea1 == 1, 1, 0)]
  tmp2[, init_na := ifelse(onset == 1 &amp;amp; sidea1 == 1, 1, NA)]

return(tmp2)
  
}

# merging and cleaning mids to dyads
monthly_mid_list &amp;lt;- mclapply(1:length(dispute_nums), make_monthly_mid, 
  dispute_dt = disputes, mc.cores = 8)
monthly_mids &amp;lt;- rbindlist(monthly_mid_list)
master_mids &amp;lt;- merge(master_dyads, monthly_mids, 
  by.x = c(&amp;quot;ccode1&amp;quot;, &amp;quot;ccode2&amp;quot;, &amp;quot;year&amp;quot;), by.y = c(&amp;quot;ccode1&amp;quot;, &amp;quot;ccode2&amp;quot;, &amp;quot;month&amp;quot;), 
  all = TRUE) # since we can have multiple mids per year, set all = TRUE
master_mids[, `:=`(
  hostlev.x = ifelse(is.na(hostlev.x), 0, hostlev.x),
  fatality = ifelse(is.na(fatality), 0, fatality),
  onset = ifelse(is.na(onset), 0, onset),
  onset_na = ifelse(is.na(onset_na) &amp;amp; is.na(dispnum), 0, onset_na),
  init = ifelse(is.na(init), 0, init),
  init_na = ifelse(is.na(init_na) &amp;amp; is.na(dispnum), 0, init_na)
  )]
test &amp;lt;- master_mids[,.N, by = .(ccode1, ccode2, year)]
save(master_mids, file = &amp;quot;master_mids.rdata&amp;quot;)


# Appendix ----
# create all dyad months
# this tooks about 20 minutes with 8 cores, as a heads up
years &amp;lt;- seq.Date(from = as.Date(&amp;quot;1946-01-01&amp;quot;), to = as.Date(&amp;quot;1980-12-31&amp;quot;), by = &amp;quot;month&amp;quot;)
dl &amp;lt;- distlist(as.Date(&amp;quot;1946-01-01&amp;quot;), type=&amp;quot;capdist&amp;quot;, useGW=FALSE)
dl2 &amp;lt;- dl[ (dl$ccode1 != dl$ccode2), ]
make_dyads &amp;lt;- function(i){
  dyad_year &amp;lt;- years[i]
  dl &amp;lt;- distlist(dyad_year, type=&amp;quot;capdist&amp;quot;, useGW=FALSE)
  dl2 &amp;lt;- dl[ (dl$ccode1 != dl$ccode2), ]
  dl2$capdist &amp;lt;- NULL
  dl2$year &amp;lt;- dyad_year
  dl2
}
master_dyads &amp;lt;- do.call(rbind, mclapply(1:length(years), make_dyads, mc.cores = 8))
# save(master_dyads, file = &amp;quot;~/Dropbox/Dissertation_data/master_dyads_1946_1980.rdata&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Generalized Propensity Score Weighting</title>
      <link>adamlauretig.github.io/post/generalized-propensity-score-weighting/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>adamlauretig.github.io/post/generalized-propensity-score-weighting/</guid>
      <description>&lt;div id=&#34;generalized-propensity-score-weighting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Generalized Propensity Score Weighting&lt;/h1&gt;
&lt;p&gt;Adam Lauretig
2018-11-03&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I’m writing this gist to better understand both the generalized propensity score, and marginal structural models, and especially, their intersection. In this, the goal is to estimate how a real-valued treatment can be estimated and understood for a time varying effect. Here, I denote treatment &lt;em&gt;A&lt;/em&gt; at time &lt;em&gt;t&lt;/em&gt; as &lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, and covariates &lt;em&gt;X&lt;/em&gt; for individual &lt;em&gt;i&lt;/em&gt; at time &lt;em&gt;t&lt;/em&gt; as &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;, &lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;. We are interested in the average treatment effect &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;A&lt;/em&gt; = 1&lt;/sub&gt; − &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;A&lt;/em&gt; = 0&lt;/sub&gt; in normal causal inference questions, however, in this case, because &lt;em&gt;A&lt;/em&gt; is continuous and we’re interested in the history of treatment on our outcome &lt;em&gt;Y&lt;/em&gt;, inference is a little more difficult. Drawing on work examining the Generalized Propensity Score (Hirano and Imbens 2004; Austin 2018), I simulate covariates, calculate inverse propensity score weights, and ultimately, estimate a reponse model.&lt;/p&gt;
&lt;p&gt;Marginal structural models were originally developed in epidemiology (Robins, Hernan, and Brumback 2000), and present a way to estimate potential outcomes for an outcome whose counterfactual depends on treatment &lt;em&gt;history&lt;/em&gt;, rather than simply a static treatment. Estimating the inverse probability of treatment weights is similar to other propensity score methods, however, these weights are then used to weight a second stage regression, where the outcome is regressed on the treatment history.&lt;/p&gt;
&lt;p&gt;This post assumes a knowledge of the Rubin Causal Model (“Potential Outcomes”), and the R package &lt;code&gt;data.table()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-up-parameters-for-simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setting up parameters for simulation&lt;/h1&gt;
&lt;p&gt;Here, I assume there are 3 covariates, 50 individuals, ten time periods, and a lag length of 1. That is, &lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;|&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;, &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt; and &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;|&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;, &lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;. We’ll generate &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;A&lt;/em&gt; in steps, for each of our 3 time periods, along with a constant unobserved confounder &lt;em&gt;U&lt;/em&gt; following Havercroft and Didelez (2012). Since my interest here is in a continuous treatment, I’ll use OLS to generate &lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, plus some Gaussian &lt;em&gt;ϵ&lt;/em&gt; ∼ &lt;em&gt;N&lt;/em&gt;(0, 1) noise. The challege here comes from incorporating the lagged treatment as well, and how it affects &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt; and &lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, which means that we have to calculate treatment values at each time &lt;em&gt;t&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(ggplot2)
library(mgcv)
set.seed(614L)
#number of covariates
p &amp;lt;- 3

# number of individuals
I &amp;lt;- 1000
ids &amp;lt;- paste0(&amp;quot;i_&amp;quot;, 1:I)

# number of time periods
t &amp;lt;- 3L

treatment_coefs &amp;lt;- matrix(rnorm(n = (2*p) + 1, mean = .5, sd = .25), nrow = 1)

# a time-invariant unobserved confounder
U &amp;lt;- matrix(runif(n = I, 0, 1), ncol= 1)
# unobserved confounder effects, 1 for each time period
U_effect &amp;lt;- rnorm(2, 1, 3)



# now, generating contemporaneous covariates
simulate_covariates &amp;lt;- function(time_period, units, columns){
  set.seed(time_period)
  X &amp;lt;- matrix(runif(units*columns, -1, 1), nrow = units)
  for(i in 1:columns){
    X[, i] &amp;lt;- X[, i] + U%*%U_effect[1]
  }
  covariates &amp;lt;- data.table(id = 1:units, time = time_period, X)
  return(covariates)
}
X1 &amp;lt;- simulate_covariates(time_period = 1, units = I, columns = p)
# now, adding our &amp;quot;lagged&amp;quot; variables. all zeroes since this is t = 1
X1[, `:=`(
  lag_1_V1 = 0,
  lag_1_V2 = 0,
  lag_1_V3 = 0,
  lag_1_treatment = 0
)]

eps &amp;lt;- matrix(rnorm(I, 0, 1), ncol = 1)

A1 &amp;lt;- as.matrix(X1[, 3:9]) %*% t(treatment_coefs) + eps
time_1_data &amp;lt;- data.table(X1[, 1:5], treatment = A1, X1[, 6:9])
setnames(time_1_data, c(&amp;quot;treatment.V1&amp;quot;), c(&amp;quot;treatment&amp;quot;))

# for generating X2 and X3
new_data_coefs &amp;lt;- rnorm(4, 1, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we’ll generate treatments for each successive time period. Since the data are conditioned on the past treatment, and the past data, simulating this will be a little funky. I’ll do each period by hand, but there’s probably a way to do this which will scale more effectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# time 2 starts
time_2_data &amp;lt;- as.data.table(time_1_data)
time_2_data[, time := time + 1]
time_2_data[, 7:10] &amp;lt;- time_2_data[, 3:6]
Xt1_temp &amp;lt;- as.matrix(time_2_data[, 7:9])
Xt2 &amp;lt;- matrix(NA, nrow = I, ncol = p)
# generate new covariates
for( i in 1:3){
 Xt2[, i] &amp;lt;- (as.matrix(Xt1_temp[, i]) %*% new_data_coefs[i]) + as.matrix(time_2_data[, 10]) %*% new_data_coefs[4] 
 Xt2[, i] &amp;lt;- Xt2[, i] + U %*% U_effect[1]
}

for(i in 1:p){
  time_2_data[, (i + 2)] &amp;lt;- Xt2[, i]
}
eps &amp;lt;- matrix(rnorm(I, 0, 1), ncol = 1)
A2 &amp;lt;- as.matrix(time_2_data[, c(3:5, 7:10)])%*% t(treatment_coefs) + eps
time_2_data[, treatment := A2]

# time 3

time_3_data &amp;lt;- as.data.table(time_2_data)
time_3_data[, time := time + 1]
time_3_data[, 7:10] &amp;lt;- time_2_data[, 3:6]
Xt3 &amp;lt;- matrix(NA, nrow = I, ncol = p)
# generate new covariates
for( i in 1:3){
 Xt3[, i] &amp;lt;- (as.matrix(Xt1_temp[, i]) %*% new_data_coefs[i]) + as.matrix(time_2_data[, 10]) %*% new_data_coefs[4] 
 Xt3[, i] &amp;lt;- Xt3[, i] + U %*% U_effect[1]
}

for(i in 1:p){
  time_3_data[, (i + 3)] &amp;lt;- Xt3[, i]
}
eps &amp;lt;- matrix(rnorm(I, 0, 1), ncol = 1)
A3 &amp;lt;- as.matrix(time_3_data[, c(3:5, 7:10)])%*% t(treatment_coefs) + eps
time_3_data[, treatment := A3]

covariates &amp;lt;- rbindlist(list(time_1_data, time_2_data, time_3_data))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we want to generate the outcome variable &lt;em&gt;Y&lt;/em&gt;. Since we have a continuous treatment &lt;em&gt;A&lt;/em&gt;, we divide the treatment into quantiles (Austin 2018), and assign a treatment effect to each quantile. We then simulate our potential outcome for each combination of quantiles over time. Here, we’ll set up terciles, split at 33% and 67%, assign treatment values, and generate our outcome. We’ll treat the treatment value below 33% as the default. I’ve chosen to hard-code the coefficients for the outcome model here, but you could replace them with &lt;code&gt;rnorm()&lt;/code&gt;. For personal reasons, I’m interested in the effect on a poisson-distributed outcome, however, as I provide the code here, feel free to modify.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cutpoints &amp;lt;- quantile(covariates$treatment, c(.33, .67))
covariates[,`:=`(
  treatment_low = ifelse(treatment &amp;lt; cutpoints[1], 1, 0),
  treatment_mid = ifelse(treatment &amp;gt;= cutpoints[1] &amp;amp; 
      treatment &amp;lt; cutpoints[2], 1, 0),
  treatment_high = ifelse(treatment &amp;gt;= cutpoints[2], 1, 0)
)]

# Now, the outcomes
# create the matrix of covariates (we&amp;#39;ll treat the low-range as the default)

treatment_history_dt &amp;lt;- dcast.data.table(covariates, id ~ time, value.var = c(&amp;quot;treatment_low&amp;quot;, &amp;quot;treatment_mid&amp;quot;, &amp;quot;treatment_high&amp;quot;))
X_matrix1 &amp;lt;- as.matrix(treatment_history_dt[, 5:10])
X_matrix1 &amp;lt;- cbind(1, X_matrix1)
# treatments for intercept, time periods 1, 2, 3 for mid and high, respectively
outcome_beta &amp;lt;- c(seq(-5, 1, 1), U_effect[2])
X_matrix &amp;lt;- cbind(X_matrix1, U)
# treatment * treatment effect, and then, multiplied by weights
Y &amp;lt;- (X_matrix%*% outcome_beta) + rnorm(I, 0, 3)
treatment_history_dt[, Y := Y]
# what if we have a poisson outcome?
treatment_history_dt[, Y_lambda := exp(Y)]
treatment_history_dt[, Y_pois := rpois(n = I, Y_lambda)]
outcome_beta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -5.000000 -4.000000 -3.000000 -2.000000 -1.000000  0.000000  1.000000
## [8]  6.617358&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;recovering-initial-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recovering initial parameters&lt;/h1&gt;
&lt;p&gt;Now we’ll see how we do at recovering our original parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;observed_data &amp;lt;- dcast.data.table(covariates, id~time, value.var = c(&amp;quot;V1&amp;quot;, &amp;quot;V2&amp;quot;, &amp;quot;V3&amp;quot;, &amp;quot;treatment&amp;quot;))
final_data &amp;lt;-  merge(treatment_history_dt, observed_data, by = &amp;quot;id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s start with a simple regression, including covariates, w/our continuous treatment:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;- glm(Y_pois ~ treatment_1 + treatment_2 + treatment_3 + 
    V1_1 + V1_2 + V1_3 + 
    V2_1 + V2_2 + V2_3 + 
    V3_1 + V3_2 + V3_3, data = final_data, family = &amp;quot;poisson&amp;quot;)
summary(m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y_pois ~ treatment_1 + treatment_2 + treatment_3 + 
##     V1_1 + V1_2 + V1_3 + V2_1 + V2_2 + V2_3 + V3_1 + V3_2 + V3_3, 
##     family = &amp;quot;poisson&amp;quot;, data = final_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -17.151   -3.857   -2.241   -1.120   74.868  
## 
## Coefficients: (5 not defined because of singularities)
##              Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  -0.61719    0.03893  -15.85   &amp;lt;2e-16 ***
## treatment_1 -10.14704    0.11127  -91.19   &amp;lt;2e-16 ***
## treatment_2  -0.35349    0.01060  -33.34   &amp;lt;2e-16 ***
## treatment_3  -0.23807    0.00923  -25.79   &amp;lt;2e-16 ***
## V1_1         11.04304    0.10240  107.84   &amp;lt;2e-16 ***
## V1_2          4.90742    0.05126   95.73   &amp;lt;2e-16 ***
## V1_3               NA         NA      NA       NA    
## V2_1         -0.83979    0.02042  -41.13   &amp;lt;2e-16 ***
## V2_2               NA         NA      NA       NA    
## V2_3               NA         NA      NA       NA    
## V3_1         -0.84228    0.02008  -41.94   &amp;lt;2e-16 ***
## V3_2               NA         NA      NA       NA    
## V3_3               NA         NA      NA       NA    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 67487  on 999  degrees of freedom
## Residual deviance: 49523  on 992  degrees of freedom
## AIC: 50341
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s use our tercile treatments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m2 &amp;lt;- glm(Y_pois ~ treatment_mid_1 + treatment_mid_2 + treatment_mid_3 + treatment_high_1 + treatment_high_2 + treatment_high_3, data = final_data, family = &amp;quot;poisson&amp;quot;)
summary(m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y_pois ~ treatment_mid_1 + treatment_mid_2 + treatment_mid_3 + 
##     treatment_high_1 + treatment_high_2 + treatment_high_3, family = &amp;quot;poisson&amp;quot;, 
##     data = final_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -10.166   -1.597   -1.378   -0.284   80.388  
## 
## Coefficients: (1 not defined because of singularities)
##                  Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)       0.25367    0.08275   3.065 0.002174 ** 
## treatment_mid_1  -3.70155    0.05651 -65.503  &amp;lt; 2e-16 ***
## treatment_mid_2  -1.24377    0.64993  -1.914 0.055658 .  
## treatment_mid_3  -2.22334    0.60918  -3.650 0.000263 ***
## treatment_high_1       NA         NA      NA       NA    
## treatment_high_2  2.75248    0.65672   4.191 2.77e-05 ***
## treatment_high_3  0.93880    0.66021   1.422 0.155032    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 67487  on 999  degrees of freedom
## Residual deviance: 39299  on 994  degrees of freedom
## AIC: 40113
## 
## Number of Fisher Scoring iterations: 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we see that our estimates in both cases of the treatment effects are quite different than &lt;code&gt;outcome_beta&lt;/code&gt; above. I’ll also point out the &lt;code&gt;NA&lt;/code&gt; for &lt;code&gt;treatment_high_1&lt;/code&gt;, as there are no treatment values at time 1 which are in the top 3&lt;sup&gt;*r**d*&lt;/sup&gt; tercile.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weighting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Weighting&lt;/h1&gt;
&lt;p&gt;However, we can weight our models with the probability of treatment, which allows us to condition on the covariates which confound selection, throughout the treatment history, a technique originally developed in (Robins, Hernan, and Brumback 2000). Here, I’m grabbing the &lt;code&gt;covariates&lt;/code&gt; data.table from above, but only keeping those variables we would expect to see in the actual dataset (mostly because the format is easier initially). It’s important to note that throughout the process of estimating the weights, we do not work with our outcome &lt;em&gt;Y&lt;/em&gt;, right now, we’re interested in &lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; = 1&lt;/sub&gt;, &lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; = 2&lt;/sub&gt;, &lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; = 3&lt;/sub&gt; and &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt; and &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;observed_covariates &amp;lt;- covariates[,.(id, time, V1, V2, V3, treatment)]
# observed_covariates &amp;lt;- observed_covariates[ id %in% sample_ids]
head(observed_covariates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id time        V1         V2        V3 treatment
## 1:  1    1 1.4772656 2.00786587 2.6898583 5.6703968
## 2:  2    1 2.1898973 2.81537133 3.3800436 5.6278032
## 3:  3    1 1.7195138 1.34037390 2.3076397 5.0996892
## 4:  4    1 3.1863947 3.27995514 2.2454097 5.9033757
## 5:  5    1 0.2455929 0.07894221 0.2261046 0.9730124
## 6:  6    1 2.9160902 1.19751095 1.2838997 4.0934380&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;observed_covariates[, normalized_treatment := (treatment - mean(treatment))/sd(treatment) ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to make our weights. Following (Austin 2018), we’ll perform OLS regression on our treatment, as our outcome is continuous. Then, for the denominator of our inverse propensity score weights, to calculate the probability of a particular treatment &lt;em&gt;a&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;, we will calculate &lt;em&gt;P&lt;strong&gt;r&lt;em&gt;(&lt;/em&gt;A&lt;em&gt; = &lt;/em&gt;a&lt;em&gt;|&lt;/em&gt;a&lt;em&gt;&lt;sup&gt;*&lt;/sup&gt;, &lt;/em&gt;σ&lt;em&gt;&lt;sub&gt;&lt;/em&gt;a&lt;em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/sub&gt;). The stabilizing numerator is &lt;/em&gt;P&lt;/strong&gt;r&lt;/em&gt;(&lt;em&gt;A&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt; = &lt;em&gt;a&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;|0, 1) (Z. Zhang et al. 2016). Here, we know that we need a single lag of our data to estimate the propensity scores. We then take the product of our stabilized weights from each time period, in order to produce individual-specific weights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;observed_covariates_lag &amp;lt;- as.data.table(observed_covariates)
observed_covariates_lag[, time := time + 1]
setnames(observed_covariates_lag, 
  c(&amp;quot;V1&amp;quot;, &amp;quot;V2&amp;quot;, &amp;quot;V3&amp;quot;, &amp;quot;treatment&amp;quot;, &amp;quot;normalized_treatment&amp;quot;), 
  c(&amp;quot;lag1_V1&amp;quot;, &amp;quot;lag1_V2&amp;quot;, &amp;quot;lag1_V3&amp;quot;, &amp;quot;lag1_treatment&amp;quot;, &amp;quot;lag1_normalized_treatment&amp;quot;))
observed_covariates_2 &amp;lt;- merge(observed_covariates, observed_covariates_lag, 
  by = c(&amp;quot;id&amp;quot;, &amp;quot;time&amp;quot;), all.x = TRUE)
observed_covariates_2[, `:=`(
  lag1_treatment = ifelse(is.na(lag1_treatment), 0, lag1_treatment),
  lag1_normalized_treatment = ifelse(
    is.na(lag1_normalized_treatment), 0, lag1_normalized_treatment),
  lag1_V1 = ifelse(is.na(lag1_V1), 0, lag1_V1),
  lag1_V2 = ifelse(is.na(lag1_V2), 0, lag1_V2),
  lag1_V3 = ifelse(is.na(lag1_V3), 0, lag1_V3)
)]

a_star_model &amp;lt;- lm(normalized_treatment ~ lag1_normalized_treatment + V1 + V2 + V3 + lag1_V1 + lag1_V2 + lag1_V3, data = observed_covariates_2)
# observed_covariates[, a_star := predict(a_star_model,type =  &amp;quot;response&amp;quot;)]
observed_covariates[, ipw_denominator := dnorm(normalized_treatment, mean = a_star_model$fitted.values, sd = sd(a_star_model$fitted.values)) ]
# observed_covariates[, dnorm(normalized_treatment, mean = a_star_model$fitted.values, sd = summary(a_star_model)$sigma) ]

observed_covariates[, ipw_numerator := dnorm(normalized_treatment, mean = 0, sd = 1) ]
observed_covariates[, ipw_weight := exp(log(ipw_numerator)-log(ipw_denominator))]


wt_cutpoints &amp;lt;- quantile(observed_covariates$ipw_weight[!(is.infinite(observed_covariates$ipw_weight))], probs = c(.01, .99), na.rm = TRUE)

observed_covariates[, ipw_weight_trunc := ifelse(ipw_weight &amp;lt; wt_cutpoints[1], wt_cutpoints[1], ipw_weight)]
observed_covariates[, ipw_weight_trunc := ifelse(ipw_weight &amp;gt; wt_cutpoints[2], wt_cutpoints[2], ipw_weight)]
# observed_covariates[, ipw_weight_trunc := ifelse(is.infinite(wt_cutpoints), wt_cutpoints[2], wt_cutpoints)]
ids_weights &amp;lt;- observed_covariates[, prod(ipw_weight_trunc), by = .(id)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll then regress our observed outcome, &lt;em&gt;Y&lt;/em&gt;, on our treatment history, which we’ll break into terciles, following the way we generated our data above. Note here that until we regress &lt;em&gt;Y&lt;/em&gt; on &lt;em&gt;A&lt;/em&gt;, we still are only focused on &lt;em&gt;A&lt;/em&gt;, &lt;em&gt;Y&lt;/em&gt; does not enter into our estimation at all.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# getting cutpoints for terciles
cut_values &amp;lt;- quantile(observed_covariates$treatment, probs = c(.33, .67))

observed_covariates[,`:=`(
  treatment_low = ifelse(treatment &amp;lt; cut_values[1], 1, 0),
  treatment_mid = ifelse(treatment &amp;gt;= cut_values[1] &amp;amp; 
      treatment &amp;lt; cut_values[2], 1, 0),
  treatment_high = ifelse(treatment &amp;gt;= cut_values[2], 1, 0)
)]

treatment_history &amp;lt;- dcast.data.table(observed_covariates, id ~ time, value.var = c(&amp;quot;treatment_low&amp;quot;, &amp;quot;treatment_mid&amp;quot;, &amp;quot;treatment_high&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a &lt;strong&gt;wide&lt;/strong&gt; dataset. We only observe &lt;em&gt;Y&lt;/em&gt; at the “end” of the treatment history, however, we have an entire sequence of treatments &lt;em&gt;A&lt;/em&gt; we regress &lt;em&gt;Y&lt;/em&gt; on. We’ll now regress &lt;em&gt;Y&lt;/em&gt; on &lt;em&gt;A&lt;/em&gt;, the treatment history, and include the weights in the &lt;code&gt;weights&lt;/code&gt; argument of &lt;code&gt;lm()&lt;/code&gt;. We’ll treat the lowest tercile of &lt;em&gt;A&lt;/em&gt; as the default, and include the “mid” and “high” terciles in the model. To include our inverse probability of treatment weights, we’ll use the &lt;code&gt;survey&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;treatment_history[, Y := final_data$Y]
treatment_history[, Y_pois := final_data$Y_pois]

treatment_history[, ipw_weights := ids_weights$V1]

library(survey)

design1 &amp;lt;- survey::svydesign(ids = ~factor(id), 
    data = treatment_history, weights = ~ (ipw_weights))

m3 &amp;lt;- survey::svyglm(Y_pois ~ treatment_mid_1 + treatment_mid_2 + treatment_mid_3 + treatment_high_1 + treatment_high_2 + treatment_high_3,  design = design1, family=quasipoisson())
summary(m3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## svyglm(formula = Y_pois ~ treatment_mid_1 + treatment_mid_2 + 
##     treatment_mid_3 + treatment_high_1 + treatment_high_2 + treatment_high_3, 
##     design = design1, family = quasipoisson())
## 
## Survey design:
## survey::svydesign(ids = ~factor(id), data = treatment_history, 
##     weights = ~(ipw_weights))
## 
## Coefficients: (1 not defined because of singularities)
##                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)       -1.7647     0.5092  -3.466 0.000552 ***
## treatment_mid_1   -3.9502     0.6039  -6.542 9.72e-11 ***
## treatment_mid_2    0.3124     1.2219   0.256 0.798279    
## treatment_mid_3   -2.3257     1.0303  -2.257 0.024199 *  
## treatment_high_2   3.5822     1.4122   2.537 0.011343 *  
## treatment_high_3   1.8727     1.4438   1.297 0.194900    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 42.8374)
## 
## Number of Fisher Scoring iterations: 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To examine how well our models perform, I calculate the mean squared error of the difference between the true coefficients, and the estimated coefficients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Mean squared error on coefficients from the naive treatment regression, and
# the ipw-weighted regression

true_coef &amp;lt;- outcome_beta[c(1:4, 6:7)]
m2_coef &amp;lt;- coef(m2)[c(1:4, 6:7)]
m3_coef &amp;lt;- na.omit(coef(m3))
mean((true_coef - m2_coef)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6.400712&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean((true_coef - m3_coef)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.856875&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While this isn’t perfect, it is much better than our naive model, which only regresses treatments on the outcome.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In this post, I illustrated how to estimate marginal structural models with a continuous treatment, and a count-valued outcome. I simulated the treatment regime, generated an outcome, and then, estimated marginal structual models from the original data. If this method seems interesting to you, I would encourage you to check out any of the previously cited works, or Blackwell and Glynn (2018).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bibliography&lt;/h1&gt;
&lt;p&gt;Austin, Peter C. 2018. “Assessing the Performance of the Generalized Propensity Score for Estimating the Effect of Quantitative or Continuous Exposures on Binary Outcomes.” &lt;em&gt;Statistics in Medicine&lt;/em&gt; 37 (11). Wiley Online Library: 1874–94.&lt;/p&gt;
&lt;p&gt;Blackwell, Matthew, and Adam Glynn. 2018. “How to Make Causal Inferences with Time-Series Cross-Sectional Data Under Selection on Observables.”&lt;/p&gt;
&lt;p&gt;Havercroft, W.G., and V. Didelez. 2012. “Simulating from Marginal Structural Models with Time-Dependent Confounding.” &lt;em&gt;Statistics in Medicine&lt;/em&gt; 31 (30). Wiley Online Library: 4190–4206.&lt;/p&gt;
&lt;p&gt;Hirano, Keisuke, and Guido W. Imbens. 2004. “The Propensity Score with Continuous Treatments.” &lt;em&gt;Applied Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives&lt;/em&gt; 226164: 73–84.&lt;/p&gt;
&lt;p&gt;Robins, James M., Miguel Angel Hernan, and Babette Brumback. 2000. “Marginal Structural Models and Causal Inference in Epidemiology.” &lt;em&gt;Epidemiology&lt;/em&gt; 11 (5).&lt;/p&gt;
&lt;p&gt;Zhang, Zhiwei, Jie Zhou, Weihua Cao, and Jun Zhang. 2016. “Causal Inference with a Quantitative Exposure.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 25 (1). SAGE Publications Sage UK: London, England: 315–35.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mapping Indonesia&#39;s Civil Service</title>
      <link>adamlauretig.github.io/policy_reports/world_bank_report/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 -0400</pubDate>
      
      <guid>adamlauretig.github.io/policy_reports/world_bank_report/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mapping Indonesia&#39;s Civil Service</title>
      <link>adamlauretig.github.io/publication/world_bank_report/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 -0400</pubDate>
      
      <guid>adamlauretig.github.io/publication/world_bank_report/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics and International Security</title>
      <link>adamlauretig.github.io/publication/stats_intl_security/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 -0400</pubDate>
      
      <guid>adamlauretig.github.io/publication/stats_intl_security/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>adamlauretig.github.io/project/example-external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>adamlauretig.github.io/project/example-external-project/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
