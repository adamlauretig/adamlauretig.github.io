<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Adam Lauretig&#39;s Professional Website</title>
    <link>/adamlauretig.github.io/post/</link>
    <description>Recent content in Posts on Adam Lauretig&#39;s Professional Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="/adamlauretig.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tokenizing on Stopwords</title>
      <link>/adamlauretig.github.io/post/tokenizing-on-stopwords/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/adamlauretig.github.io/post/tokenizing-on-stopwords/</guid>
      <description>Introduction Recently, I came across the idea that you can get relevant keywords for word2vec by tokenizing a corpus on stopwords, in addition to standard punctuation (found via link). This seemed like a really cool unsupervised way of capturing (hopefully!) relevant phrases. I was intrigued.
A brief note: “tokenizing” refers to splitting a document into words or phrases based on a pre-defined set of rules. The most common way to do this is by splitting on spaces and “end-of-sentence” punctuation (ex: “!</description>
    </item>
    
    <item>
      <title>Monthly Militarized Interstate Dispute (MID) Panel </title>
      <link>/adamlauretig.github.io/post/monthly-militarized-interstate-dispute-mid-panel/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/adamlauretig.github.io/post/monthly-militarized-interstate-dispute-mid-panel/</guid>
      <description>Just a brief note. For my dissertation, I needed to create a monthly panel of militarized interstate dispute (MID) data, which proved to be more difficult than initially anticipated. To “be kind to my future self,” I wrote up what I did on a gist, and I’ll also copy the code below. If you find yourself in a similar situation, I hope this helps!
# Code to create a monthly, directed dyadic panel of # Militarized Interstate Dispute data, with directional initiation # uses cshapes to create monthly list of all dyads, then uses data.</description>
    </item>
    
    <item>
      <title>Generalized Propensity Score Weighting</title>
      <link>/adamlauretig.github.io/post/generalized-propensity-score-weighting/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/adamlauretig.github.io/post/generalized-propensity-score-weighting/</guid>
      <description>Generalized Propensity Score Weighting Adam Lauretig 2018-11-03
 Introduction I’m writing this gist to better understand both the generalized propensity score, and marginal structural models, and especially, their intersection. In this, the goal is to estimate how a real-valued treatment can be estimated and understood for a time varying effect. Here, I denote treatment A at time t as At, and covariates X for individual i at time t as Xi, t.</description>
    </item>
    
  </channel>
</rss>