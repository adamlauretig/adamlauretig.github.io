<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Adam Lauretig</title>
    <link>adamlauretig.github.io/post/</link>
    <description>Recent content in Posts on Adam Lauretig</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="adamlauretig.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bayesian Factorization Machines in Stan</title>
      <link>adamlauretig.github.io/post/factorization-machines/</link>
      <pubDate>Sat, 30 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>adamlauretig.github.io/post/factorization-machines/</guid>
      <description>Introduction I’ve recently become interested in factorization machines, a flexible augmentation of regression, which assumes the interactions between two variables factorizes into a low-rank matrix, the coefficient on the interaction is the result of a dot-product on latent factors. This structure regularizes coefficients, allows for non-linear higher-order interactions, and is computationally cheap to compute. It is a flexible, scalable, and interpretable model, useful for a variety of applications.
However, resources for learning about and implementing factorization machines in R are scant, and resources for learning about and implementing Bayesian factorization machines even more so.</description>
    </item>
    
    <item>
      <title>Tokenizing on Stopwords</title>
      <link>adamlauretig.github.io/post/tokenizing-on-stopwords/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>adamlauretig.github.io/post/tokenizing-on-stopwords/</guid>
      <description>Introduction Recently, I came across the idea that you can get relevant keywords for word2vec by tokenizing a corpus on stopwords, in addition to standard punctuation (found via). This seemed like a really cool unsupervised way of capturing (hopefully!) relevant phrases. I was intrigued.
A brief note: “tokenizing” refers to splitting a document into words or phrases based on a pre-defined set of rules. The most common way to do this is by splitting on spaces and “end-of-sentence” punctuation (ex: “!</description>
    </item>
    
    <item>
      <title>Monthly Militarized Interstate Dispute (MID) Panel </title>
      <link>adamlauretig.github.io/post/monthly-militarized-interstate-dispute-mid-panel/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>adamlauretig.github.io/post/monthly-militarized-interstate-dispute-mid-panel/</guid>
      <description>Just a brief note. For my dissertation, I needed to create a monthly panel of militarized interstate dispute (MID) data, which proved to be more difficult than initially anticipated. To “be kind to my future self,” I wrote up what I did on a gist, and I’ll also copy the code below. If you find yourself in a similar situation, I hope this helps!
# Code to create a monthly, directed dyadic panel of # Militarized Interstate Dispute data, with directional initiation # uses cshapes to create monthly list of all dyads, then uses data.</description>
    </item>
    
    <item>
      <title>Generalized Propensity Score Weighting</title>
      <link>adamlauretig.github.io/post/generalized-propensity-score-weighting/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>adamlauretig.github.io/post/generalized-propensity-score-weighting/</guid>
      <description>Generalized Propensity Score Weighting Adam Lauretig 2018-11-03
 Introduction I’m writing this gist to better understand both the generalized propensity score, and marginal structural models, and especially, their intersection. In this, the goal is to estimate how a real-valued treatment can be estimated and understood for a time varying effect. Here, I denote treatment A at time t as At, and covariates X for individual i at time t as Xi, t.</description>
    </item>
    
  </channel>
</rss>